{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpZXGp1rQ_jZ"
   },
   "source": [
    "# **FULLY CONECTED NET FROM SCRATCH (MLP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH5f8KSMRNDJ"
   },
   "source": [
    "This program performs a fully conected net but only using elemental elements, no pytorch...\n",
    "The dataset is going to be MNIST\n",
    "Last modification: 11/08/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1755631013298,
     "user": {
      "displayName": "Marc Moré Puig (MarcMoré)",
      "userId": "09392263257885655311"
     },
     "user_tz": -120
    },
    "id": "a1XyA0usRLlc",
    "outputId": "973da747-d36b-4608-c653-711adc62817d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 784)\n",
      "Test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Open the dataset\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "print(\"Train shape:\", x_train.shape)\n",
    "print(\"Test shape:\", x_test.shape)\n",
    "\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "y_test_onehot  = np.eye(num_classes)[y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3Nh5IkXSLW0"
   },
   "source": [
    "Now, we are going to define all the class we will need for our MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1755631019148,
     "user": {
      "displayName": "Marc Moré Puig (MarcMoré)",
      "userId": "09392263257885655311"
     },
     "user_tz": -120
    },
    "id": "UulsQqZwSUst"
   },
   "outputs": [],
   "source": [
    "class LayerDense:\n",
    "  def __init__(self, num_inputs, num_neurons):\n",
    "    self.weight = 0.1* np.random.randn(num_inputs, num_neurons)\n",
    "    self.bias = np.zeros((1,num_neurons))\n",
    "    self.dweight = np.zeros(self.weight.shape)\n",
    "    self.dbias = np.zeros(self.bias.shape)\n",
    "    self.dinputs = None\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = np.dot(inputs, self.weight) + self.bias\n",
    "\n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = np.dot(dvalues,self.weight.T)\n",
    "    self.dweight = np.dot(self.inputs.T, dvalues)\n",
    "    self.dbias = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "class ActivationReLU:\n",
    "  def __init__(self):\n",
    "    self.inputs = None\n",
    "    self.output = None\n",
    "    self.dinputs = None\n",
    "\n",
    "  def forward(self,inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = np.maximum(0, inputs)\n",
    "\n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = dvalues.copy()\n",
    "    self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class ActivationSoftmax:\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = np.exp(inputs)/ np.exp(inputs).sum(axis = 1, keepdims = True)\n",
    "\n",
    "class LossFunction:\n",
    "  def __init__(self):\n",
    "    self.loss = None\n",
    "    self.gradient = None\n",
    "    self.delta = 1e-7\n",
    "\n",
    "  def forward(self, y_true, y_pred):\n",
    "    #To avoid 0, because the log will perform an infinite number\n",
    "    stability = y_pred + self.delta\n",
    "    # Cross entropy loss:\n",
    "    sample_losses = -np.sum(y_true * np.log(stability), axis=1)\n",
    "    self.loss = np.mean(sample_losses)\n",
    "\n",
    "  def backward(self , y_true, y_pred):\n",
    "    self.gradient = (y_pred - y_true)/y_true.shape[0]\n",
    "\n",
    "class Optimizer:\n",
    "  def __init__(self, learning_rate):\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "  def update(self, layer):\n",
    "    layer.weight -= self.learning_rate * layer.dweight\n",
    "    layer.bias -= self.learning_rate * layer.dbias\n",
    "\n",
    "\n",
    "\n",
    "class Mlp:\n",
    "  def __init__(self, numInputs, hidden_layers, learningRate, batchSize, epochs, numOutputs ):\n",
    "\n",
    "\n",
    "    if numOutputs <= 1:\n",
    "        raise ValueError(\"The number of outputs must be greater than 1 for classification\")\n",
    "\n",
    "    self.layers = []\n",
    "    self.numInputs = numInputs\n",
    "    self.hidden_layers = hidden_layers\n",
    "    self.learningRate = learningRate\n",
    "    self.batchSize = batchSize\n",
    "    self.epochs = epochs\n",
    "    self.loss = LossFunction()\n",
    "    self.prediction = None\n",
    "    self.optimizer = Optimizer(self.learningRate)\n",
    "    self.firstForward = False\n",
    "\n",
    "\n",
    "    for i in self.hidden_layers[:-1]:\n",
    "      self.layers.append(LayerDense(self.numInputs, i))\n",
    "      self.layers.append(ActivationReLU())\n",
    "      self.numInputs = i\n",
    "\n",
    "    if self.hidden_layers:\n",
    "      self.layers.append(LayerDense(self.numInputs, self.hidden_layers[-1]))\n",
    "      self.layers.append(ActivationReLU())\n",
    "      self.numInputs = self.hidden_layers[-1]\n",
    "\n",
    "    self.layers.append(LayerDense(self.numInputs, numOutputs))\n",
    "    self.layers.append(ActivationSoftmax())\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    if not self.firstForward:\n",
    "      self.firstForward = True\n",
    "\n",
    "    for layer in self.layers:\n",
    "      layer.forward(inputs)\n",
    "      inputs = layer.output\n",
    "\n",
    "    # Self.prediction is the last prediction of the net\n",
    "    self.prediction = inputs\n",
    "\n",
    "  def backward(self, y_true):\n",
    "\n",
    "    # Step1 Calculate the actual gradient\n",
    "\n",
    "    self.loss.backward(y_true, self.prediction)\n",
    "    dvalues = self.loss.gradient\n",
    "\n",
    "    # Step 2, run over all the layers except the softmac\n",
    "    for layer in reversed(self.layers[:-1]):\n",
    "      if hasattr(layer, \"backward\"):\n",
    "        layer.backward(dvalues)\n",
    "        dvalues = layer.dinputs\n",
    "\n",
    "  def train(self, x_train, y_train):\n",
    "    numBatch = int(np.ceil(x_train.shape[0] / self.batchSize))\n",
    "\n",
    "    for epoch in range(self.epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_index in range(numBatch):\n",
    "            start = batch_index * self.batchSize\n",
    "            end = min(start + self.batchSize, x_train.shape[0])\n",
    "\n",
    "            x_batch = x_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Forward\n",
    "            self.forward(x_batch)\n",
    "            self.loss.forward(y_batch, self.prediction)\n",
    "            epoch_loss += self.loss.loss * (end - start)\n",
    "\n",
    "            # Backward\n",
    "            self.backward(y_batch)\n",
    "\n",
    "            # Optimize only the Dense layers\n",
    "            for layer in self.layers:\n",
    "                if isinstance(layer, LayerDense):\n",
    "                    self.optimizer.update(layer)\n",
    "\n",
    "        epoch_loss /= x_train.shape[0]\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "  def predict(self, x_test):\n",
    "    self.forward(x_test)\n",
    "    return np.argmax(self.prediction, axis=1)\n",
    "\n",
    "  def evaluate(self, x_test, y_test):\n",
    "    predictions = self.predict(x_test)\n",
    "    y_true_index = np.argmax(y_test, axis=1)\n",
    "    accuracy = np.mean(predictions == y_true_index)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqL3kfrow3Zu"
   },
   "source": [
    "Let's do the training to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27829,
     "status": "ok",
     "timestamp": 1755631091980,
     "user": {
      "displayName": "Marc Moré Puig (MarcMoré)",
      "userId": "09392263257885655311"
     },
     "user_tz": -120
    },
    "id": "eBj2OTVxw6B_",
    "outputId": "0d6c9ce3-3f3e-4036-dae0-071944cd6b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0152\n",
      "Epoch 2/10, Loss: 0.4143\n",
      "Epoch 3/10, Loss: 0.3384\n",
      "Epoch 4/10, Loss: 0.2998\n",
      "Epoch 5/10, Loss: 0.2728\n",
      "Epoch 6/10, Loss: 0.2515\n",
      "Epoch 7/10, Loss: 0.2338\n",
      "Epoch 8/10, Loss: 0.2188\n",
      "Epoch 9/10, Loss: 0.2057\n",
      "Epoch 10/10, Loss: 0.1942\n",
      "Test accuracy: 0.9442\n"
     ]
    }
   ],
   "source": [
    "mlp = Mlp(\n",
    "    numInputs=784,\n",
    "    hidden_layers=[128, 64],\n",
    "    learningRate=0.01,\n",
    "    batchSize=64,\n",
    "    epochs=10,\n",
    "    numOutputs=10\n",
    ")\n",
    "\n",
    "mlp.train(x_train, y_train_onehot)\n",
    "accuracy = mlp.evaluate(x_test, y_test_onehot)\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPlUWXukOzLeI6nayhi7AfK",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
